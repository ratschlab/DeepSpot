{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da1eea45-93e6-43ca-9147-ac156ce13688",
   "metadata": {},
   "source": [
    "## DeepSpot inference from H&E images\n",
    "\n",
    "Here, we provide an example of how to use the pretrained weights and perform inference using DeepSpot to predict spatial transcriptomics from H&E images.\n",
    "\n",
    "You can download the pretrained weights at https://zenodo.org/records/15322099"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6441929b-4e1c-4f31-b62f-093b7d78c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389a0163-39b4-43ca-9b51-3e50e894c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### download from zenodo\n",
    "!wget -c https://zenodo.org/records/15322099/files/DeepSpot_pretrained_model_weights.zip?download=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d93697-4916-42b7-ba0f-9ded1398e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "### unzip data\n",
    "!unzip DeepSpot_pretrained_model_weights.zip?download\=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ef0d7a-3e8d-4932-a3a0-e121962e9e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "### you should see the available weights listed\n",
    "!ls -al DeepSpot_pretrained_model_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd210287-0500-4447-9efb-d2c0be5e74be",
   "metadata": {},
   "source": [
    "Export packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7379e96d-02c3-4dc6-aaa1-5fe23eb25c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepspot.utils.utils_image import predict_spot_spatial_transcriptomics_from_image_path\n",
    "from deepspot.utils.utils_image import get_morphology_model_and_preprocess\n",
    "from deepspot.utils.utils_image import crop_tile\n",
    "\n",
    "from deepspot.spot import DeepSpot\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "from openslide import open_slide\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import squidpy as sq\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyvips\n",
    "import torch\n",
    "import math\n",
    "import yaml\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e15f58-bce4-45b8-a9a0-ae2b9e8feae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb569dc1-3d7f-4af7-9210-7ef5580be625",
   "metadata": {},
   "source": [
    "Here, we specify the input parameters. This information should be selected carefully, as it is based on the spatial transcriptomics training dataset. We continue with values based on our toy example from the COAD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168fd039-3a55-4f45-acd0-569609d862fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = \"example_data\"\n",
    "white_cutoff = 200  # recommended, but feel free to explore\n",
    "downsample_factor = 10 # downsampling the image used for visualisation in squidpy\n",
    "model_weights = 'DeepSpot_pretrained_model_weights/Colon_HEST1K/final_model.pkl'\n",
    "model_hparam = 'DeepSpot_pretrained_model_weights/Colon_HEST1K/top_param_overall.yaml'\n",
    "gene_path = 'DeepSpot_pretrained_model_weights/Colon_HEST1K/info_highly_variable_genes.csv'\n",
    "sample = 'ZEN38'\n",
    "image_path = f'example_data/data/image/{sample}_without_fud.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b6533-6886-444c-9410-4e07c2a41569",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_hparam, \"r\") as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "config\n",
    "\n",
    "# the model is trained on H&E images at 20x magnification and Visium with tissue area of 55um\n",
    "# NB: please adapt the parameters based on your data \n",
    "# n_mini_tiles is always 9\n",
    "# spot_diameter and spot_distance are based on your H&E resolution\n",
    "n_mini_tiles = 9 # number of non-overlaping subspots\n",
    "spot_diameter = 100#config['spot_diameter'] # spot diameter\n",
    "spot_distance = 100#config['spot_distance'] # distance between spots\n",
    "image_feature_model = config['image_feature_model'] \n",
    "image_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df1bb61-e610-4cbf-8e52-86d84bfd097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f18123d-06f5-4fd7-977f-2390c1d8458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Specify the weights for the pretrained model used for tile feature extraction\n",
    "image_feature_model_path = \"../huggingface/hub/models--MahmoodLab--UNI/blobs/56ef09b44a25dc5c7eedc55551b3d47bcd17659a7a33837cf9abc9ec4e2ffb40\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7f3fd8-a41e-4516-9616-8239866e7634",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = pd.read_csv(gene_path)\n",
    "selected_genes_bool = genes.isPredicted.values\n",
    "genes_to_predict = genes[selected_genes_bool]\n",
    "genes_to_predict.sort_values(\"highly_variable_rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26facee3-018a-4667-9361-c1bb64e89369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "image = mpimg.imread(image_path)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79ab5d5-5d5f-409d-8a41-a11a716a93fd",
   "metadata": {},
   "source": [
    "We build the grid coordinates for the spots based on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af7756-95f5-445a-a2a6-4954911220b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pyvips.Image.new_from_file(image_path)\n",
    "\n",
    "coord = []\n",
    "for i, x in enumerate(range(spot_diameter + 1, image.height - spot_diameter - 1, spot_distance)):\n",
    "    for j, y in enumerate(range(spot_diameter + 1, image.width - spot_diameter - 1, spot_distance)):\n",
    "        coord.append([i, j, x, y])\n",
    "coord = pd.DataFrame(coord, columns=['x_array', 'y_array', 'x_pixel', 'y_pixel'])\n",
    "coord.index = coord.index.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d22b812-4c4b-433d-9c20-289fc9bda853",
   "metadata": {},
   "source": [
    "We select the spots that are located within the tissue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac0c831-4bcf-4388-ad06-952596a04b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_white = []\n",
    "counts = []\n",
    "for _, row in tqdm(coord.iterrows()):\n",
    "    x = row.x_pixel - int(spot_diameter // 2)\n",
    "    y = row.y_pixel - int(spot_diameter // 2)\n",
    "    \n",
    "    main_tile = crop_tile(image, x, y, spot_diameter)\n",
    "    main_tile = main_tile[:,:,:3]\n",
    "    white = np.mean(main_tile)\n",
    "    is_white.append(white)\n",
    "\n",
    "counts = np.empty((len(is_white), selected_genes_bool.sum())) # empty count matrix \n",
    "\n",
    "coord['is_white'] = is_white"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60a45ee-55ef-446a-9274-38e6ba0d17fa",
   "metadata": {},
   "source": [
    "We create the anndata object, empty for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f1962d-356c-44d1-8be0-e96210ca1917",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = ad.AnnData(counts)\n",
    "adata.var.index = genes[selected_genes_bool].gene_name.values\n",
    "adata.obs = adata.obs.merge(coord, left_index=True, right_index=True)\n",
    "adata.obs['is_white'] = coord['is_white'].values\n",
    "adata.obs['is_white_bool'] = (coord['is_white'].values > white_cutoff).astype(int)\n",
    "adata.obs['sampleID'] = sample\n",
    "adata.obs['barcode'] = adata.obs.index\n",
    "adata = adata[adata.obs.is_white_bool == 0, ]\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c1561e-d0d4-4b19-8b47-9f0e75d25fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE IMAGE\n",
    "img = open_slide(image_path)\n",
    "n_level = len(img.level_dimensions) - 1 # 0 based\n",
    "\n",
    "\n",
    "large_w, large_h = img.dimensions\n",
    "new_w = math.floor(large_w / downsample_factor)\n",
    "new_h = math.floor(large_h / downsample_factor)\n",
    "\n",
    "whole_slide_image = img.read_region((0, 0), n_level, img.level_dimensions[-1])\n",
    "whole_slide_image = whole_slide_image.convert(\"RGB\")\n",
    "img_downsample = whole_slide_image.resize((new_w, new_h), PIL.Image.BILINEAR)\n",
    "\n",
    "\n",
    "adata.obsm['spatial'] = adata.obs[[\"y_pixel\", \"x_pixel\"]].values\n",
    "# adjust coordinates to new image dimensions\n",
    "adata.obsm['spatial'] = adata.obsm['spatial'] / downsample_factor\n",
    "# create 'spatial' entries\n",
    "adata.uns['spatial'] = dict()\n",
    "adata.uns['spatial']['library_id'] = dict()\n",
    "adata.uns['spatial']['library_id']['images'] = dict()\n",
    "adata.uns['spatial']['library_id']['images']['hires'] = np.array(img_downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbedf95-7f96-4638-8364-05a7a06d667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YAML file into a regular Python dictionary\n",
    "with open(model_hparam, 'r') as yaml_file:\n",
    "    model_hparam = yaml.safe_load(yaml_file)\n",
    "model_hparam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc94bb5c-243a-4cc1-9ebb-4c774c7ed49a",
   "metadata": {},
   "source": [
    "Initialize DeepSpot and the pretrained pathology foundation model. This time, we compute the tile representation on the fly, which may take more time. The current implementation preprocesses a single spot per batch, but extending it to multiple spots might offer additional speed improvements. Contributions are welcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b1dc5c-556c-4ea6-8b53-96ae555f4246",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_expression = torch.load(model_weights, map_location=device)\n",
    "model_expression.to(device)\n",
    "model_expression.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ade376-cc19-4c44-b283-46173ca253cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "morphology_model, preprocess, feature_dim = get_morphology_model_and_preprocess(model_name=image_feature_model, \n",
    "                                                                                device=device, model_path=image_feature_model_path)\n",
    "morphology_model.to(device)\n",
    "morphology_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ab934-e87e-4845-8a28-487e539ddc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = predict_spot_spatial_transcriptomics_from_image_path(image_path, \n",
    "                                                        adata,\n",
    "                                                        spot_diameter,\n",
    "                                                        n_mini_tiles,\n",
    "                                                        preprocess, \n",
    "                                                        morphology_model, \n",
    "                                                        model_expression, \n",
    "                                                        device,\n",
    "                                                        super_resolution=False,\n",
    "                                                        neighbor_radius=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cbfcb2-1bce-44b1-ac56-d68bfec9150c",
   "metadata": {},
   "source": [
    "##### Remember from the training notebook...\n",
    "The `scaler` is important to be the same as the one used during training, so that the predictions of DeepSpot can be rescaled back to their original ranges using the `inverse_transform` function. \n",
    "\n",
    "##### IMPORTANT: Remember to manually rescale the values, as this is not done automatically.\n",
    "```\n",
    "expression_norm = model(X)\n",
    "expression_norm should be np.array\n",
    "expression = model.inverse_transform(expression_norm)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c24b42d-a110-4f83-ad1d-04631ffd4996",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = model_expression.inverse_transform(counts)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3802bd20-43f5-44ca-b867-42cc38f3c16f",
   "metadata": {},
   "source": [
    "You are free to explore other types of transformations that may enhance spatial transcriptomics predictions. The following are just a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d2d02-169a-4192-8158-ecd03f4d0bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[counts < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ec9bd1-ea92-4d13-a6e8-8b7e1c6bed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_predicted = ad.AnnData(counts, \n",
    "                             var=adata.var,\n",
    "                             obs=adata.obs, \n",
    "                             uns=adata.uns, \n",
    "                             obsm=adata.obsm).copy()\n",
    "adata_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36535275-d43b-4f8a-82b0-ea2030af0369",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq.pl.spatial_scatter(adata_predicted, \n",
    "                      color=['MUC2', 'ITLN1', \n",
    "                             'CLCA1', 'FCGBP'], \n",
    "                      wspace=0,\n",
    "                      ncols=2,\n",
    "                      size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4300f0a4-6842-4be8-a4df-e8612d6873d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eee229-7126-464d-aa97-25d6fb3fabf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52518a9b-89ce-4810-9f9d-d83ce67f542d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9866fb-1077-4391-9fee-0e9aae21e963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nonchev",
   "language": "python",
   "name": "nonchev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
